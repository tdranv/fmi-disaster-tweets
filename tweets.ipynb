{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Motivation\r\n",
    "\r\n",
    "This notebook's goal is to exlore a set of tweets and build an algorithm around it, that understands whether a tweet is about a real disaster or not. It accomplishes that via Natural Language Processing, which is a combination of machine learning techniques with text and stastistical approaches to transform that text in a format that the machine learning algorithms can understand."
   ],
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 0. Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import tokenization\r\n",
    "from tqdm import tqdm\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "from nltk.corpus import stopwords\r\n",
    "import tensorflow_hub as hub\r\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\r\n",
    "from tensorflow.keras.models import Model\r\n",
    "from tensorflow.keras.optimizers import Adam\r\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\r\n",
    "import tensorflow.compat.v1 as tf\r\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\r\n",
    "from keras.preprocessing.sequence import pad_sequences\r\n",
    "from keras.preprocessing.text import Tokenizer\r\n",
    "from keras.initializers import Constant\r\n",
    "from keras.models import Sequential\r\n",
    "from keras import layers, metrics, optimizers, losses\r\n",
    "import matplotlib.patches as mpatches\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib\r\n",
    "import re\r\n",
    "import string\r\n",
    "import unicodedata\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "from collections import defaultdict\r\n",
    "\r\n",
    "\r\n",
    "plt.style.use('seaborn-bright')\r\n",
    "\r\n",
    "random_state_split = 42\r\n",
    "Dropout_num = 0\r\n",
    "learning_rate = 5.95e-6\r\n",
    "valid = 0.15\r\n",
    "epochs_num = 3\r\n",
    "batch_size_num = 16\r\n",
    "target_corrected = False\r\n",
    "target_big_corrected = False\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1. Exploring the data\r\n",
    "\r\n",
    "We will use `pandas` to exlore the datasets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.1 Training Set Data Preview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "training = pd.read_csv(\"./train.csv\")\r\n",
    "test = pd.read_csv(\"./test.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Training set preview\r\n",
    "training.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.2 Test Set Data Preview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Test set preview\r\n",
    "test.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.3 Training Set Format"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "training.info()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Breakdown of the columns in the training dataset:\r\n",
    "\r\n",
    "- id: A unique identifier for each training.\r\n",
    "- keyword: A particular keyword from the training (can be blank).\r\n",
    "- location: The location the training was sent from (can be blank).\r\n",
    "- text: The text of the training.\r\n",
    "- target: `1` indicates that the training is about a real disaster, `0` that it is not."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.4 Exploring the dimensions of the datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print('there are {} rows and {} columns in test.csv'.format(test.shape[0], test.shape[1]))\r\n",
    "print('there are {} rows and {} columns in training.csv'.format(training.shape[0], training.shape[1]))"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.5 Visualizing the target classes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(12, 6))\r\n",
    "plt.title(\"Count of the target classes\")\r\n",
    "sns.countplot(y=training[\"target\"], linewidth=1, palette='Set2')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*There are around 3200 samples of tweets about real disasters and about 4500 non-disaster tweets.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.6 Analyzing the length of the tweets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "disaster_tweets = training[training['target'] == 1]['text'].str\r\n",
    "non_disaster_tweets = training[training['target'] == 0]['text'].str\r\n",
    "\r\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 7))\r\n",
    "\r\n",
    "char_len_dis = disaster_tweets.len()\r\n",
    "ax1.hist(char_len_dis, color='orange', edgecolor='black', linewidth=0.7)\r\n",
    "ax1.set_title('disaster tweets')\r\n",
    "\r\n",
    "char_len_ndis = non_disaster_tweets.len()\r\n",
    "ax2.hist(char_len_ndis, color='green', edgecolor='black', linewidth=0.7)\r\n",
    "ax2.set_title('non-disaster tweets')\r\n",
    "\r\n",
    "plt.suptitle(\"Characters per training\")\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*The characters count of disaster and non-disaster tweets are between 120 and 140.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.7 Analyzing the number of words in the tweets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\r\n",
    "\r\n",
    "char_len_dis = disaster_tweets.split().map(lambda x: len(x))\r\n",
    "ax1.hist(char_len_dis, color='orange', edgecolor='black', linewidth=0.7)\r\n",
    "ax1.set_title('disaster tweets')\r\n",
    "\r\n",
    "char_len_ndis = non_disaster_tweets.split().map(lambda x: len(x))\r\n",
    "ax2.hist(char_len_ndis, color='green', edgecolor='black', linewidth=0.7)\r\n",
    "ax2.set_title('non-disaster tweets')\r\n",
    "\r\n",
    "plt.suptitle(\"length of words in text\")\r\n",
    "plt.tight_layout()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*The number of words in disaster and non-disaster tweets are in the range 15 - 20.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.8 Analysis of the average length of words in each tweet"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\r\n",
    "\r\n",
    "char_len_dis = disaster_tweets.split().apply(lambda x: [len(i) for i in x])\r\n",
    "sns.distplot(char_len_dis.map(lambda x: np.mean(x)), ax=ax1, color='orange')\r\n",
    "ax1.set_title('disaster tweets')\r\n",
    "\r\n",
    "char_len_ndis = non_disaster_tweets.split().apply(lambda x: [len(i) for i in x])\r\n",
    "sns.distplot(char_len_ndis.map(lambda x: np.mean(x)), ax=ax2, color='green')\r\n",
    "ax2.set_title('non-disaster tweets')\r\n",
    "\r\n",
    "plt.suptitle(\"Average length of words in tweets\")\r\n",
    "plt.tight_layout()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*The average word count for disaster tweets is in the range 7 - 7.5, while for non-disaster tweets in the range of 4.5 - 5.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Creating a corpus for further analysis.\r\n",
    "def create_corpus(target):\r\n",
    "    corpus = []\r\n",
    "    for x in training[training['target'] == target]['text'].str.split():\r\n",
    "        corpus.extend(x)\r\n",
    "    return corpus"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.9 Analysing the top stop words in text."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def analyse_stopwords(func, target):\r\n",
    "    values_list = []\r\n",
    "    for labels in range(0, len(target)):\r\n",
    "        dic = defaultdict(int)\r\n",
    "        corpus = func(target[labels])\r\n",
    "        for word in corpus:\r\n",
    "            dic[word] += 1\r\n",
    "        top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\r\n",
    "        x_items, y_values = zip(*top)\r\n",
    "        values_list.append(x_items)\r\n",
    "        values_list.append(y_values)\r\n",
    "\r\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 7))\r\n",
    "    ax1.barh(values_list[0], values_list[1], color=\"lightblue\", edgecolor='black', linewidth=1.2)\r\n",
    "    ax1.set_title(\"non-disaster tweets\")\r\n",
    "\r\n",
    "    ax2.barh(values_list[2], values_list[3], color=\"lightgreen\", edgecolor='black', linewidth=1.2)\r\n",
    "    ax2.set_title(\"disaster tweets\")\r\n",
    "\r\n",
    "    plt.suptitle(\"Top stop words in the dataset\")\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "\r\n",
    "analyse_stopwords(create_corpus, [0, 1])"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*From the bar chart we can observe that the most frequently occurring stopwords in both disaster/non-disaster tweets is \"the\" (1000+ occurrences) while the least occurring for non-disaster is \"for\" (400+ occurrences) and for disaster tweets is \"is\" (300+ occurrences).*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.10 Analysing punctuation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def analyse_punctuation(func, target):\r\n",
    "    values_list = []\r\n",
    "    special = string.punctuation\r\n",
    "    for labels in range(0, len(target)):\r\n",
    "        dic = defaultdict(int)\r\n",
    "        corpus = func(target[labels])\r\n",
    "        for i in corpus:\r\n",
    "            if i in special:\r\n",
    "                dic[i] += 1\r\n",
    "        x_items, y_values = zip(*dic.items())\r\n",
    "        values_list.append(x_items)\r\n",
    "        values_list.append(y_values)\r\n",
    "\r\n",
    "    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\r\n",
    "    ax1.bar(values_list[0], values_list[1],\r\n",
    "            color=\"lightblue\", edgecolor='black', linewidth=0.7)\r\n",
    "    ax1.set_title(\"non-disaster tweets\")\r\n",
    "\r\n",
    "    ax2.bar(values_list[2], values_list[3],\r\n",
    "            color=\"lightgreen\", edgecolor='black', linewidth=0.7)\r\n",
    "    ax2.set_title(\"disaster tweets\")\r\n",
    "\r\n",
    "    plt.suptitle(\"punctuation in tweets\")\r\n",
    "    plt.show()\r\n",
    "\r\n",
    "\r\n",
    "analyse_punctuation(create_corpus, [0, 1])\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*From the bar chart we can see that the punctuation with the most occurrences in both disaster/non-disaster tweets is \"-\" (350+) while the ones with the least occurrences for non-disaster are \"%\", \":\", \"$\", \"_\" and for disaster tweets they are \"=>\" and \")\".*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.11 Missing values analysis\r\n",
    "\r\n",
    "Both the training and test datasets have the same missing values for `location` and `keyword`.\r\n",
    "\r\n",
    "- **2.3% - 2.4%** missing keywords in boths sets\r\n",
    "- **97.6% - 97.7%** missing locations in both sets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "missing_train = training.isnull().sum()\r\n",
    "missing_test = test.isnull().sum()\r\n",
    "_, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\r\n",
    "missing_train = missing_train[missing_train > 0].sort_values()\r\n",
    "ax1.pie(missing_train, autopct='%1.1f%%', startangle=30, explode=[0.9, 0], labels=[\"keyword\", \"location\"],\r\n",
    "        colors=['yellow', 'cyan'])\r\n",
    "ax1.set_title(\"null values present in the training dataset\")\r\n",
    "\r\n",
    "missing_test = missing_test[missing_test > 0].sort_values()\r\n",
    "ax2.pie(missing_test, autopct='%1.1f%%', startangle=30, explode=[0.9, 0], labels=[\"keyword\", \"location\"],\r\n",
    "        colors=['yellow', '#66ff00'])\r\n",
    "ax2.set_title(\"null values present in the test dataset\")\r\n",
    "\r\n",
    "plt.suptitle(\"Distribution of null values in the dataset\")\r\n",
    "plt.tight_layout()\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*The above pictorial representation displays the missing values in each of the datasets. From the distribution, it is observed that columns `keyword` and `location` contain missing values. For the training data, the % of missing values is 97.6 for `location` and 24 for `keyword`, while for the testing dataset, it is 97.7% for `location` and 23% for `keyword`. Also, the column having maximum missing values is: `location` while `keyword` column has the minimum count of missing values for both sets of data.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.11 Analysing the top 20 disastrous keywords"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 7))\r\n",
    "training[training['target'] == 1]['keyword'].value_counts()[:20].plot(kind='barh', fontsize=12, color='#0096FF',\r\n",
    "                                                                      linewidth=0.7,\r\n",
    "                                                                      title='Top 20 keywords in disastrous tweets')\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*From the above bar chart is evident that `outbrak`, `wreckage` and `derailment` are the most frequent keywords in disastrous tweets with close to 40 occurrences.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.12 Analysing the top 20 disastrous locations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plt.figure(figsize=(10, 7))\r\n",
    "training[training[\"target\"] == 1][\"location\"].value_counts()[:20].plot(kind='barh', fontsize=12, color='#0096FF',\r\n",
    "                                                                       linewidth=0.7,\r\n",
    "                                                                       title='Top 20 most frequent locations for disastrous tweets')\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*From the above bar chart we can see that, from the tweets that do have a location, the United States is the most frequent.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. Cleaning up the data\r\n",
    "\r\n",
    "There are some words/characters/strings that need to be removed and formatted. We will be using the Natural Language Toolkit's stopwords set and WordNetLemmatizer to group together different forms of words in order for them to be analsyed as one.\r\n",
    "\r\n",
    "Here is what operation we will execute over each entry:\r\n",
    "\r\n",
    "- Remove all URL's.\r\n",
    "- Remove all Emoji's.\r\n",
    "- Remove all HTMl tags.\r\n",
    "- Lowercase all text.\r\n",
    "- Words shorter than 2 symbols.\r\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lemmatizer = WordNetLemmatizer()\r\n",
    "stop_words = set(stopwords.words('english'))\r\n",
    "stop_words.update(string.punctuation)\r\n",
    "\r\n",
    "\r\n",
    "def remove_emoji(text):\r\n",
    "    emoji_pattern = re.compile(\"[\"\r\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\r\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\r\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\r\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\r\n",
    "                               u\"\\U00002702-\\U000027B0\"\r\n",
    "                               u\"\\U000024C2-\\U0001F251\"\r\n",
    "                               \"]+\", flags=re.UNICODE)\r\n",
    "    return emoji_pattern.sub(r'', text)\r\n",
    "\r\n",
    "\r\n",
    "def remove_punct(text):\r\n",
    "    table = str.maketrans('', '', string.punctuation)\r\n",
    "    return text.translate(table)\r\n",
    "\r\n",
    "\r\n",
    "def cleanup_text(texts):\r\n",
    "    corpus = list()\r\n",
    "    for text in texts:\r\n",
    "\r\n",
    "        # remove non-ascii characters\r\n",
    "        text = unicodedata.normalize('NFKD', text).encode(\r\n",
    "            'ascii', 'ignore').decode('utf-8', 'ignore')\r\n",
    "\r\n",
    "        # remove html tags\r\n",
    "        text = re.sub(r'<.*?>', ' ', text)\r\n",
    "\r\n",
    "        # remove urls\r\n",
    "        text = re.sub(r'https?://\\S+|www\\.\\S+|http?://\\S+', ' ', text)\r\n",
    "\r\n",
    "        # remove emojis\r\n",
    "        text = remove_emoji(text)\r\n",
    "\r\n",
    "        # remove  punctuation\r\n",
    "        text = remove_punct(text)\r\n",
    "\r\n",
    "        # keeping only alphabetic characters\r\n",
    "        text = re.sub(r'[^a-zA-Z]', ' ', text)\r\n",
    "\r\n",
    "        text = text.lower()\r\n",
    "        text = text.split()\r\n",
    "\r\n",
    "        # remove all words, shorter than 2 characters\r\n",
    "        text = [i for i in text if len(i) > 2]\r\n",
    "\r\n",
    "        # remove stopwords from text\r\n",
    "        final_text = []\r\n",
    "        for word in text:\r\n",
    "            word = word.strip()\r\n",
    "            if word not in stop_words:\r\n",
    "                final_text.append(word)\r\n",
    "\r\n",
    "        text = \" \".join(final_text)\r\n",
    "\r\n",
    "        text = \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\r\n",
    "        text = \" \".join([lemmatizer.lemmatize(word, pos='v')\r\n",
    "                        for word in text.split()])\r\n",
    "\r\n",
    "        corpus.append(text)\r\n",
    "\r\n",
    "    return corpus\r\n",
    "\r\n",
    "\r\n",
    "training['clean_text'] = cleanup_text(training['text'])\r\n",
    "test['clean_text'] = cleanup_text(test['text'])\r\n",
    "training.head()\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*We can observe the effect of the cleanup and how the algorithm leaves only the important words from the training.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sample_df = training.sample(n=10).reset_index(drop=True)\r\n",
    "for i in range(3):\r\n",
    "    print(\"-\" * 100)\r\n",
    "    print(f\"BEFORE: {sample_df.loc[i, 'text']}\\n\")\r\n",
    "    print(f\"AFTER: {sample_df.loc[i, 'clean_text']}\")"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. Visualizing the data\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.1 Analyse the top 20 words in the training data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "disaster_tweet_clean = training[training.target == 1][\"clean_text\"]\r\n",
    "non_disaster_tweet_clean = training[training.target == 0][\"clean_text\"]\r\n",
    "\r\n",
    "color = ['Paired', 'Accent']\r\n",
    "dataSplit = [disaster_tweet_clean, non_disaster_tweet_clean]\r\n",
    "title = [\"disaster tweets\", \"non-disaster tweets\"]\r\n",
    "for item in range(2):\r\n",
    "    plt.figure(figsize=(20, 8))\r\n",
    "    plt.title(title[item], fontsize=14)\r\n",
    "    pd.Series(' '.join([i for i in dataSplit[item]]).split()).value_counts().head(20).plot(kind='barh', fontsize=14,\r\n",
    "                                                                                           colormap=color[item],\r\n",
    "                                                                                           edgecolor='black',\r\n",
    "                                                                                           linewidth=0.7)\r\n",
    "    plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*`fire` seems to be the most frequent word among the disaster tweets (+270), while `like` is the most frequent for the non-disaster tweets (+260). Other frequent words in the disaster tweets are `news`, `amp`, and `disaster`. For the non-disaster tweets `amp`, `get`, and `new` are also frequent.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.2 Further cleanup\r\n",
    "\r\n",
    "From the above graphs we see that, although we cleaned the data, there are still some unnecessary words left, such as `like`, `amp`, `get`. We will now remove them."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "common_words = ['via', 'like', 'build', 'get', 'would', 'one', 'two', 'feel', 'lol', 'fuck', 'take', 'way', 'may',\r\n",
    "                'first', 'latest', 'want', 'make', 'back', 'see', 'know', 'let', 'look', 'come', 'got', 'still', 'say',\r\n",
    "                'think', 'great', 'pleas', 'amp']\r\n",
    "\r\n",
    "\r\n",
    "def text_cleaning(data):\r\n",
    "    return ' '.join(i for i in data.split() if i not in common_words)\r\n",
    "\r\n",
    "\r\n",
    "training[\"clean_text\"] = training[\"clean_text\"].apply(text_cleaning)\r\n",
    "test[\"clean_text\"] = test[\"clean_text\"].apply(text_cleaning)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's review the data now"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.3 Top 20 words in the training data (after a thorough cleanup)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "disaster_tweet_clean = training[training.target == 1][\"clean_text\"]\r\n",
    "non_disaster_tweet_clean = training[training.target == 0][\"clean_text\"]\r\n",
    "\r\n",
    "color = ['Paired', 'Accent']\r\n",
    "dataSplit = [disaster_tweet_clean, non_disaster_tweet_clean]\r\n",
    "title = [\"disaster tweets\", \"non-disaster tweets\"]\r\n",
    "for item in range(2):\r\n",
    "    plt.figure(figsize=(20, 8))\r\n",
    "    plt.title(title[item], fontsize=14)\r\n",
    "    pd.Series(' '.join([i for i in dataSplit[item]]).split()).value_counts().head(20).plot(kind='barh', fontsize=14,\r\n",
    "                                                                                           colormap=color[item],\r\n",
    "                                                                                           edgecolor='black',\r\n",
    "                                                                                           linewidth=0.7)\r\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*After the cleanup, some the most frequent words in the disaster tweets dataset are `fire`, `news`, `disaster`, `california`, `suicide`. From the non-disaster tweets, the most frequent are: `new`, `body`, `time`, `day`.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.4 Plotting Common N-grams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def top_n_grams(data, top_grams_num, grams):\r\n",
    "    word_freq = []\r\n",
    "    if grams == 2:\r\n",
    "        count_vec = CountVectorizer(ngram_range=(2, 2)).fit(data)\r\n",
    "        bow = count_vec.transform(data)\r\n",
    "        add_words = bow.sum(axis=0)\r\n",
    "        word_freq = [(word, add_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\r\n",
    "        word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\r\n",
    "    elif grams == 3:\r\n",
    "        count_vec = CountVectorizer(ngram_range=(3, 3)).fit(data)\r\n",
    "        bow = count_vec.transform(data)\r\n",
    "        add_words = bow.sum(axis=0)\r\n",
    "        word_freq = [(word, add_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\r\n",
    "        word_freq = sorted(word_freq, key=lambda x: x[1], reverse=True)\r\n",
    "\r\n",
    "    return word_freq[:top_grams_num]"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.1 Bigrams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "disaster_bigrams = top_n_grams(training[training['target'] == 1][\"clean_text\"], 10, 2)\r\n",
    "non_disaster_bigrams = top_n_grams(training[training['target'] == 0][\"clean_text\"], 10, 2)\r\n",
    "\r\n",
    "disaster_bigrams_df = pd.DataFrame(disaster_bigrams, columns=['word', 'freq'])\r\n",
    "non_disaster_bigrams_df = pd.DataFrame(non_disaster_bigrams, columns=['word', 'freq'])\r\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 20))\r\n",
    "\r\n",
    "ax1.bar(disaster_bigrams_df[\"word\"], disaster_bigrams_df[\"freq\"], color=\"lightblue\", edgecolor='black', linewidth=0.7)\r\n",
    "ax1.set_title(\"Top 10 disaster bigrams in the dataset\")\r\n",
    "ax1.set_xlabel(\"Words\")\r\n",
    "ax1.set_ylabel(\"Frequency\")\r\n",
    "ax1.set_xticklabels(rotation=90, labels=disaster_bigrams_df[\"word\"], fontsize=14)\r\n",
    "\r\n",
    "ax2.bar(non_disaster_bigrams_df[\"word\"], non_disaster_bigrams_df[\"freq\"], color=\"lightgreen\", edgecolor='black',\r\n",
    "        linewidth=0.7)\r\n",
    "ax2.set_title(\"Top 10 non-disaster bigrams in the dataset.\")\r\n",
    "ax2.set_xlabel(\"Words\")\r\n",
    "ax2.set_ylabel(\"Frequency\")\r\n",
    "ax2.set_xticklabels(rotation=90, labels=non_disaster_bigrams_df[\"word\"], fontsize=14)\r\n",
    "plt.tight_layout(pad=1.85)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*`suicide bomber`, `northern california`, and `oil spill` are among the most frequent bigrams in the disaster tweets dataset, while `liked youtube`, `cross body`, and `youtube video` are among the most common bigrams in the non-disaster dataset.*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.4.2 Trigrams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "disaster_trigrams = top_n_grams(training[training['target'] == 1][\"clean_text\"], 10, 3)\r\n",
    "non_disaster_trigrams = top_n_grams(training[training['target'] == 0][\"clean_text\"], 10, 3)\r\n",
    "\r\n",
    "disaster_trigrams_df = pd.DataFrame(disaster_trigrams, columns=['word', 'freq'])\r\n",
    "non_disaster_trigrams_df = pd.DataFrame(non_disaster_trigrams, columns=['word', 'freq'])\r\n",
    "_, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 20))\r\n",
    "\r\n",
    "ax1.bar(disaster_trigrams_df[\"word\"], disaster_trigrams_df[\"freq\"], color=\"lightblue\", edgecolor='black', linewidth=0.7)\r\n",
    "ax1.set_title(\"Top 10 disaster trigrams in the dataset\")\r\n",
    "ax1.set_xlabel(\"Words\")\r\n",
    "ax1.set_ylabel(\"Frequency\")\r\n",
    "ax1.set_xticklabels(rotation=90, labels=disaster_trigrams_df[\"word\"], fontsize=14)\r\n",
    "\r\n",
    "ax2.bar(non_disaster_trigrams_df[\"word\"], non_disaster_trigrams_df[\"freq\"], color=\"lightgreen\", edgecolor='black',\r\n",
    "        linewidth=0.7)\r\n",
    "ax2.set_title(\"Top 10 non-disaster trigrams in the dataset.\")\r\n",
    "ax2.set_xlabel(\"Words\")\r\n",
    "ax2.set_ylabel(\"Frequency\")\r\n",
    "ax2.set_xticklabels(rotation=90, labels=non_disaster_trigrams_df[\"word\"], fontsize=14)\r\n",
    "plt.tight_layout(pad=1.85)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*When it comes to trigrams, `suicide bomber detonated`, and `northern california wildfire` are among the most frequent ones in the disaster tweets dataset, while `liked youtube video`, `cross body bag`, and `reddit quarantine offensive` are among the most common in the non-disaster dataset.*"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4. Creating the models\r\n",
    "\r\n",
    "A natural way to represent text for computers is to encode each character individually, this seems quite inadequate to represent and understand language. Our goal is to first create a useful embedding for each sentence (or tweet) in our dataset, and then use these embeddings to accurately predict the relevant category.\r\n",
    "\r\n",
    "The simplest approach we can start with is to use a bag of words model, and apply a logistic regression on top. A bag of words just associates an index to each word in our vocabulary, and embeds each sentence as a list of 0s, with a 1 at each index corresponding to a word present in the sentence.\r\n",
    "\r\n",
    "*Credit: [NLP Tutorial](https://github.com/hundredblocks/concrete_NLP_tutorial/blob/master/NLP_notebook.ipynb)*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.1 Bag of Words Counts"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def cv(data):\r\n",
    "    count_vectorizer = CountVectorizer()\r\n",
    "\r\n",
    "    emb = count_vectorizer.fit_transform(data)\r\n",
    "\r\n",
    "    return emb, count_vectorizer\r\n",
    "\r\n",
    "list_corpus = training[\"clean_text\"].tolist()\r\n",
    "list_labels = training[\"target\"].tolist()\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, \r\n",
    "                                                                                random_state=random_state_split)\r\n",
    "\r\n",
    "X_train_counts, count_vectorizer = cv(X_train)\r\n",
    "X_test_counts = count_vectorizer.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualizing the embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_LSA(test_data, test_labels, plot=True):\r\n",
    "    lsa = TruncatedSVD(n_components=2)\r\n",
    "    lsa.fit(test_data)\r\n",
    "    lsa_scores = lsa.transform(test_data)\r\n",
    "    color_mapper = {label: idx for idx, label in enumerate(set(test_labels))}\r\n",
    "    color_column = [color_mapper[label] for label in test_labels]\r\n",
    "    colors = ['orange', 'blue']\r\n",
    "    if plot:\r\n",
    "        plt.scatter(lsa_scores[:, 0], lsa_scores[:, 1], s=8, alpha=.8,\r\n",
    "                    c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))\r\n",
    "        orange_patch = mpatches.Patch(color='orange', label='Not')\r\n",
    "        blue_patch = mpatches.Patch(color='blue', label='Real')\r\n",
    "        plt.legend(handles=[orange_patch, blue_patch], prop={'size': 30})\r\n",
    "\r\n",
    "\r\n",
    "fig = plt.figure(figsize=(16, 16))\r\n",
    "plot_LSA(X_train_counts, y_train)\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The embeddings do not look very clearly separated. Let's see if we can do something about it."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.2 Term Frequency â€” Inverse Document Frequency"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def tfidf(data):\r\n",
    "    tfidf_vectorizer = TfidfVectorizer()\r\n",
    "\r\n",
    "    train = tfidf_vectorizer.fit_transform(data)\r\n",
    "\r\n",
    "    return train, tfidf_vectorizer\r\n",
    "\r\n",
    "X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\r\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(16, 16))          \r\n",
    "plot_LSA(X_train_tfidf, y_train)\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.3 GloVe\r\n",
    "\r\n",
    "We will use GloVe pretrained corpus model to represent our words.\r\n",
    "\r\n",
    "[Credit](https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove#GloVe-for-Vectorization)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def create_corpus(df):\r\n",
    "    corpus=[]\r\n",
    "    for tweet in tqdm(df['clean_text']):\r\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if(word.isalpha()==1)]\r\n",
    "        corpus.append(words)\r\n",
    "    return corpus"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "corpus=create_corpus(training)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embedding_dict={}\r\n",
    "with open('./glove.6B.100d.txt', encoding='utf-8') as f:\r\n",
    "    for line in f:\r\n",
    "        values=line.split()\r\n",
    "        word=values[0]\r\n",
    "        vectors=np.asarray(values[1:],'float32')\r\n",
    "        embedding_dict[word]=vectors\r\n",
    "f.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "MAX_LEN=50\r\n",
    "tokenizer_obj=Tokenizer()\r\n",
    "tokenizer_obj.fit_on_texts(corpus)\r\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\r\n",
    "\r\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "word_index=tokenizer_obj.word_index\r\n",
    "print('Number of unique words:',len(word_index))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_words=len(word_index)+1\r\n",
    "embedding_matrix=np.zeros((num_words,100))\r\n",
    "\r\n",
    "for word,i in tqdm(word_index.items()):\r\n",
    "    if i < num_words:\r\n",
    "        emb_vec=embedding_dict.get(word)\r\n",
    "        if emb_vec is not None:\r\n",
    "            embedding_matrix[i]=emb_vec      "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "tweet_pad[0][0:]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Baseline model with GloVe"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = Sequential()\r\n",
    "\r\n",
    "embedding = Embedding(num_words, 100, embeddings_initializer=Constant(embedding_matrix),\r\n",
    "                      input_length=MAX_LEN, trainable=False)\r\n",
    "\r\n",
    "model.add(embedding)\r\n",
    "model.add(SpatialDropout1D(0.2))\r\n",
    "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\r\n",
    "model.add(Dense(1, activation='sigmoid'))\r\n",
    "\r\n",
    "\r\n",
    "optimzer = Adam(learning_rate=3e-4)\r\n",
    "\r\n",
    "model.compile(loss='binary_crossentropy',\r\n",
    "              optimizer=optimzer, metrics=['accuracy'])\r\n",
    "\r\n",
    "model.summary()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_v = tweet_pad[:training.shape[0]]\r\n",
    "test_v = tweet_pad[training.shape[0]:]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\r\n",
    "    train_v, training['target'].values, test_size=0.15)\r\n",
    "print('Shape of train', X_train.shape)\r\n",
    "print(\"Shape of Validation \", X_test.shape)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fig = plt.figure(figsize=(16, 16))\r\n",
    "plot_LSA(train_v, training['target'])\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "history = model.fit(X_train, y_train, batch_size=4, epochs=10,\r\n",
    "                    validation_data=(X_test, y_test), verbose=2)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_pred_GloVe = model.predict(train_v)\r\n",
    "train_pred_GloVe_int = train_pred_GloVe.round().astype('int')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.4 BERT using TFHub\r\n",
    "\r\n",
    "[Creadits](https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\r\n",
    "    all_tokens = []\r\n",
    "    all_masks = []\r\n",
    "    all_segments = []\r\n",
    "\r\n",
    "    for text in texts:\r\n",
    "        text = tokenizer.tokenize(text)\r\n",
    "\r\n",
    "        text = text[:max_len-2]\r\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\r\n",
    "        pad_len = max_len - len(input_sequence)\r\n",
    "\r\n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\r\n",
    "        tokens += [0] * pad_len\r\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\r\n",
    "        segment_ids = [0] * max_len\r\n",
    "\r\n",
    "        all_tokens.append(tokens)\r\n",
    "        all_masks.append(pad_masks)\r\n",
    "        all_segments.append(segment_ids)\r\n",
    "\r\n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def build_model(bert_layer, max_len=512):\r\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\r\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\r\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\r\n",
    "\r\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\r\n",
    "    clf_output = sequence_output[:, 0, :]\r\n",
    "    \r\n",
    "    if Dropout_num == 0:\r\n",
    "        # Without Dropout\r\n",
    "        out = Dense(1, activation='sigmoid')(clf_output)\r\n",
    "    else:\r\n",
    "        # With Dropout(Dropout_num), Dropout_num > 0\r\n",
    "        x = Dropout(Dropout_num)(clf_output)\r\n",
    "        out = Dense(1, activation='sigmoid')(x)\r\n",
    "\r\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\r\n",
    "    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\r\n",
    "    \r\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Load BERT from the Tensorflow Hub\r\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\r\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build and train BERT model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Load tokenizer from the bert layer\r\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\r\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\r\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# Encode the text into tokens, masks, and segment flags\r\n",
    "train_input = bert_encode(training.text.values, tokenizer, max_len=160)\r\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\r\n",
    "train_labels = training.target.values\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "# Build BERT model\r\n",
    "model_BERT = build_model(bert_layer, max_len=160)\r\n",
    "model_BERT.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (None, 1024)         0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf.__operators__.getitem[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\todor\\anaconda3\\envs\\disaster_tweets\\lib\\site-packages\\keras\\optimizer_v2\\optimizer_v2.py:356: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "checkpoint = ModelCheckpoint(\r\n",
    "    'model_BERT.h5', monitor='val_loss', save_best_only=True)\r\n",
    "\r\n",
    "train_history = model_BERT.fit(\r\n",
    "    train_input, train_labels,\r\n",
    "    validation_split=valid,\r\n",
    "    epochs=epochs_num,  # recomended 3-5 epochs\r\n",
    "    callbacks=[checkpoint],\r\n",
    "    batch_size=batch_size_num\r\n",
    ")\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prediction by BERT model\r\n",
    "model_BERT.load_weights('model_BERT.h5')\r\n",
    "test_pred_BERT = model_BERT.predict(test_input)\r\n",
    "test_pred_BERT_int = test_pred_BERT.round().astype('int')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Prediction by BERT model for the training data - for the Confusion Matrix\r\n",
    "train_pred_BERT = model_BERT.predict(train_input)\r\n",
    "train_pred_BERT_int = train_pred_BERT.round().astype('int')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "pred = pd.DataFrame(test_pred_BERT, columns=['preds'])\r\n",
    "pred.plot.hist()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.5 Showing confusion matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_cm(y_true, y_pred, title, figsize=(5, 5)):\r\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\r\n",
    "    cm_sum = np.sum(cm, axis=1, keepdims=True)\r\n",
    "    cm_perc = cm / cm_sum.astype(float) * 100\r\n",
    "    annot = np.empty_like(cm).astype(str)\r\n",
    "    nrows, ncols = cm.shape\r\n",
    "    for i in range(nrows):\r\n",
    "        for j in range(ncols):\r\n",
    "            c = cm[i, j]\r\n",
    "            p = cm_perc[i, j]\r\n",
    "            if i == j:\r\n",
    "                s = cm_sum[i]\r\n",
    "                annot[i, j] = '%.1f%%\\n%d/%d' % (p, c, s)\r\n",
    "            elif c == 0:\r\n",
    "                annot[i, j] = ''\r\n",
    "            else:\r\n",
    "                annot[i, j] = '%.1f%%\\n%d' % (p, c)\r\n",
    "    cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\r\n",
    "    cm.index.name = 'Actual'\r\n",
    "    cm.columns.name = 'Predicted'\r\n",
    "    fig, ax = plt.subplots(figsize=figsize)\r\n",
    "    plt.title(title)\r\n",
    "    sns.heatmap(cm, cmap=\"YlGnBu\", annot=annot, fmt='', ax=ax)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Showing Confusion Matrix for the GloVe model\r\n",
    "plot_cm(train_pred_GloVe_int, train['target'].values,\r\n",
    "        'Confusion matrix for GloVe model', figsize=(7, 7))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Showing Confusion Matrix for the BERT model\r\n",
    "plot_cm(train_pred_BERT_int, train['target'].values,\r\n",
    "        'Confusion matrix for BERT model', figsize=(7, 7))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('disaster_tweets': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "785b8e496fba5d05dfaa274b20109619faf2b061e40519c8fcf923a71ab9a250"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}